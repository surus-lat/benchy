{
  "name": "spanish",
  "display_name": "Spanish language",
  "description": "Spanish language evaluation benchmark",
  "task_format": "grouped",
  "runner_entrypoint": "src.tasks.spanish.run:run_spanish",
  "provider_types": [
    "openai",
    "anthropic",
    "surus",
    "together"
  ],
  "pipeline_overrides": {
    "set_api_endpoint": true,
    "set_generation_config": true
  },
  "tasks": [
    "copa_es",
    "escola",
    "mgsm_direct_es_spanish_bench",
    "openbookqa_es",
    "paws_es_spanish_bench",
    "teleia_pce",
    "teleia_cervantes_ave",
    "teleia_siele",
    "wnli_es"
  ],
  "task_configs": {
    "copa_es": {
      "dataset_path": "BSC-LT/COPA-es",
      "split": "test"
    },
    "escola": {
      "dataset_path": "nbel/EsCoLA",
      "split": "validation"
    },
    "mgsm_direct_es_spanish_bench": {
      "dataset_path": "juletxara/mgsm",
      "dataset_name": "es",
      "split": "test"
    },
    "openbookqa_es": {
      "dataset_path": "BSC-LT/openbookqa-es",
      "split": "test"
    },
    "paws_es_spanish_bench": {
      "dataset_path": "paws-x",
      "dataset_name": "es",
      "split": "test"
    },
    "teleia_pce": {
      "dataset_path": "migonsa/teleia",
      "split": "test"
    },
    "teleia_cervantes_ave": {
      "dataset_path": "migonsa/teleia",
      "split": "test"
    },
    "teleia_siele": {
      "dataset_path": "migonsa/teleia",
      "split": "test"
    },
    "wnli_es": {
      "dataset_path": "PlanTL-GOB-ES/wnli-es",
      "split": "validation"
    }
  },
  "prompts": {
    "system": "",
    "user": ""
  },
  "defaults": {
    "batch_size": 20,
    "log_samples": false,
    "temperature": 0.0,
    "max_tokens": 512,
    "timeout": 120,
    "max_retries": 3
  },
  "output": {
    "subdirectory": "spanish"
  },
  "metrics_manifest": [
    "acc",
    "exact_match",
    "error_rate"
  ],
  "group": "latam_es",
  "group_metadata": {
    "name": "Spanish LATAM",
    "description": "Spanish language tasks for Latin America",
    "description_en": "Spanish language tasks for Latin America",
    "description_es": "Tareas en espa\u00f1ol para Am\u00e9rica Latina",
    "description_pt": "Tarefas em espanhol para a Am\u00e9rica Latina",
    "long_description": "Suite of selected tasks from the Spanish Bench available in the lm-evaluation-harness from the team at IberoBench (https://aclanthology.org/2025.coling-main.699/) and SomosNLP's Spanish Leaderboard (https://github.com/somosnlp/lm-evaluation-harness) designed to evaluate the performance of models in the Spanish language. The tasks cover a wide range of linguistic abilities, from basic comprehension to complex reasoning in Spanish. The evaluation suite includes tasks like COPA for choice of plausible alternatives, ESCOLA for Spanish Corpus of Linguistic Acceptability, MGSM for Multilingual Grade School Math, OpenBookQA for open-domain question answering, PAWS for paraphrase adversaries from word scrambling, TELEIA for Teleia Spanish language assessment, WNLI for Winograd Natural Language Inference, and XNLI for Cross-lingual Natural Language Inference. This comprehensive set of benchmarks helps assess how well language models can process and generate Spanish text across different contexts and difficulty levels.",
    "long_description_en": "Suite of selected tasks from the Spanish Bench available in the lm-evaluation-harness from the team at IberoBench (https://aclanthology.org/2025.coling-main.699/) and SomosNLP's Spanish Leaderboard (https://github.com/somosnlp/lm-evaluation-harness) designed to evaluate the performance of models in the Spanish language. The tasks cover a wide range of linguistic abilities, from basic comprehension to complex reasoning in Spanish. The evaluation suite includes tasks like COPA for choice of plausible alternatives, ESCOLA for Spanish Corpus of Linguistic Acceptability, MGSM for Multilingual Grade School Math, OpenBookQA for open-domain question answering, PAWS for paraphrase adversaries from word scrambling, TELEIA for Teleia Spanish language assessment, WNLI for Winograd Natural Language Inference, and XNLI for Cross-lingual Natural Language Inference. This comprehensive set of benchmarks helps assess how well language models can process and generate Spanish text across different contexts and difficulty levels.",
    "long_description_es": "Conjunto de tareas seleccionadas del Spanish Bench del lm-evaluation-harness del equipo de IberoBench (https://aclanthology.org/2025.coling-main.699/) y del Spanish Leaderboard de SomosNLP (https://github.com/somosnlp/lm-evaluation-harness), dise\u00f1ado para evaluar el desempe\u00f1o de modelos en lengua espa\u00f1ola. Cubre habilidades desde comprensi\u00f3n b\u00e1sica hasta razonamiento complejo. Incluye COPA (elecci\u00f3n de alternativas plausibles), EsCoLA (aceptabilidad ling\u00fc\u00edstica), MGSM (matem\u00e1tica escolar multiling\u00fce), OpenBookQA (preguntas de ciencia de dominio abierto), PAWS (par\u00e1frasis con alta superposici\u00f3n l\u00e9xica), TELEIA (evaluaci\u00f3n del espa\u00f1ol), WNLI (inferencia Winograd) y XNLI (inferencia multiling\u00fce). Este conjunto permite medir c\u00f3mo los modelos procesan y generan texto en espa\u00f1ol en diversos contextos y niveles de dificultad.",
    "long_description_pt": "Conjunto de tarefas selecionadas do Spanish Bench no lm-evaluation-harness da equipe IberoBench (https://aclanthology.org/2025.coling-main.699/) e do Spanish Leaderboard da SomosNLP (https://github.com/somosnlp/lm-evaluation-harness), projetado para avaliar o desempenho de modelos em l\u00edngua espanhola. Cobre habilidades desde compreens\u00e3o b\u00e1sica at\u00e9 racioc\u00ednio complexo. Inclui COPA (escolha de alternativas plaus\u00edveis), EsCoLA (aceitabilidade lingu\u00edstica), MGSM (matem\u00e1tica escolar multil\u00edngue), OpenBookQA (perguntas de ci\u00eancia de dom\u00ednio aberto), PAWS (par\u00e1frases com alta sobreposi\u00e7\u00e3o lexical), TELEIA (avalia\u00e7\u00e3o do espanhol), WNLI (infer\u00eancia Winograd) e XNLI (infer\u00eancia multil\u00edngue). Esse conjunto mede como os modelos processam e geram texto em espanhol em diferentes contextos e n\u00edveis de dificuldade.",
    "repository": "https://github.com/surus-lat/lm-evaluation-harness-surus"
  },
  "task_metadata": {
    "copa_es": {
      "description": "Spanish COPA (Choice of Plausible Alternatives)",
      "description_en": "Spanish COPA (Choice of Plausible Alternatives)",
      "description_es": "COPA en espa\u00f1ol (elecci\u00f3n de alternativas plausibles)",
      "description_pt": "COPA em espanhol (escolha de alternativas plaus\u00edveis)",
      "long_description": "Spanish version of COPA, a commonsense causal reasoning benchmark. Given a premise and a relation (cause/effect), the model selects the more plausible alternative in Spanish.",
      "long_description_en": "Spanish version of COPA, a commonsense causal reasoning benchmark. Given a premise and a relation (cause/effect), the model selects the more plausible alternative in Spanish.",
      "long_description_es": "Versi\u00f3n en espa\u00f1ol de COPA, un benchmark de razonamiento causal de sentido com\u00fan. Dada una premisa y una relaci\u00f3n (causa/efecto), el modelo elige la alternativa m\u00e1s plausible en espa\u00f1ol.",
      "long_description_pt": "Vers\u00e3o em espanhol do COPA, benchmark de racioc\u00ednio causal de senso comum. Dada uma premissa e uma rela\u00e7\u00e3o (causa/efeito), o modelo escolhe a alternativa mais plaus\u00edvel em espanhol.",
      "fewshot": 0,
      "URL": "https://huggingface.co/datasets/BSC-LT/COPA-es"
    },
    "escola": {
      "description": "Spanish EsCoLA (Spanish Corpus of Linguistic Acceptability)",
      "description_en": "Spanish EsCoLA (Spanish Corpus of Linguistic Acceptability)",
      "description_es": "EsCoLA en espa\u00f1ol (Corpus de Aceptabilidad Ling\u00fc\u00edstica en Espa\u00f1ol)",
      "description_pt": "EsCoLA em espanhol (Corpus de Aceitabilidade Lingu\u00edstica em Espanhol)",
      "long_description": "EsCoLA contains Spanish sentences annotated for linguistic acceptability. The task is a binary judgment (acceptable vs. unacceptable), probing grammatical knowledge and fluency in Spanish.",
      "long_description_en": "EsCoLA contains Spanish sentences annotated for linguistic acceptability. The task is a binary judgment (acceptable vs. unacceptable), probing grammatical knowledge and fluency in Spanish.",
      "long_description_es": "EsCoLA contiene oraciones en espa\u00f1ol anotadas por aceptabilidad ling\u00fc\u00edstica. Es un juicio binario (aceptable vs. inaceptable) que explora conocimiento gramatical y fluidez.",
      "long_description_pt": "EsCoLA cont\u00e9m frases em espanhol anotadas quanto \u00e0 aceitabilidade lingu\u00edstica. A tarefa \u00e9 um julgamento bin\u00e1rio (aceit\u00e1vel vs. inaceit\u00e1vel) que explora conhecimento gramatical e flu\u00eancia.",
      "fewshot": 0,
      "URL": "https://huggingface.co/datasets/nbel/EsCoLA"
    },
    "mgsm_direct_es_spanish_bench": {
      "description": "Spanish MGSM (Multilingual Grade School Math)",
      "description_en": "Spanish MGSM (Multilingual Grade School Math)",
      "description_es": "MGSM en espa\u00f1ol (matem\u00e1tica escolar multiling\u00fce)",
      "description_pt": "MGSM em espanhol (matem\u00e1tica escolar multil\u00edngue)",
      "long_description": "Spanish subset of MGSM, a multilingual grade-school math benchmark. This configuration expects direct numeric answers (no chain-of-thought), evaluating arithmetic and reasoning in Spanish.",
      "long_description_en": "Spanish subset of MGSM, a multilingual grade-school math benchmark. This configuration expects direct numeric answers (no chain-of-thought), evaluating arithmetic and reasoning in Spanish.",
      "long_description_es": "Subconjunto en espa\u00f1ol de MGSM, benchmark multiling\u00fce de matem\u00e1tica escolar. Esta configuraci\u00f3n espera respuestas num\u00e9ricas directas (sin cadena de razonamiento), evaluando aritm\u00e9tica y razonamiento en espa\u00f1ol.",
      "long_description_pt": "Subconjunto em espanhol do MGSM, benchmark multil\u00edngue de matem\u00e1tica escolar. Esta configura\u00e7\u00e3o espera respostas num\u00e9ricas diretas (sem cadeia de racioc\u00ednio), avaliando aritm\u00e9tica e racioc\u00ednio em espanhol.",
      "fewshot": 0,
      "URL": "https://huggingface.co/datasets/juletxara/mgsm"
    },
    "openbookqa_es": {
      "description": "Spanish OpenBookQA",
      "description_en": "Spanish OpenBookQA",
      "description_es": "OpenBookQA en espa\u00f1ol",
      "description_pt": "OpenBookQA em espanhol",
      "long_description": "Spanish adaptation of OpenBookQA, a multiple-choice science QA benchmark requiring use of elementary science facts plus commonsense reasoning, localized to Spanish.",
      "long_description_en": "Spanish adaptation of OpenBookQA, a multiple-choice science QA benchmark requiring use of elementary science facts plus commonsense reasoning, localized to Spanish.",
      "long_description_es": "Adaptaci\u00f3n al espa\u00f1ol de OpenBookQA, benchmark de preguntas de ciencia de opci\u00f3n m\u00faltiple que requiere usar hechos cient\u00edficos elementales y razonamiento de sentido com\u00fan.",
      "long_description_pt": "Adapta\u00e7\u00e3o em espanhol do OpenBookQA, benchmark de perguntas de ci\u00eancia de m\u00faltipla escolha que exige fatos cient\u00edficos b\u00e1sicos e racioc\u00ednio de senso comum.",
      "fewshot": 0,
      "URL": "https://huggingface.co/datasets/BSC-LT/openbookqa-es"
    },
    "paws_es_spanish_bench": {
      "description": "Spanish PAWS (Paraphrase Adversaries from Word Scrambling)",
      "description_en": "Spanish PAWS (Paraphrase Adversaries from Word Scrambling)",
      "description_es": "PAWS en espa\u00f1ol (par\u00e1frasis con alta superposici\u00f3n l\u00e9xica)",
      "description_pt": "PAWS em espanhol (par\u00e1frases com alta sobreposi\u00e7\u00e3o lexical)",
      "long_description": "PAWS-X Spanish evaluates paraphrase identification with high-lexical-overlap sentence pairs. The model must determine whether two Spanish sentences are paraphrases.",
      "long_description_en": "PAWS-X Spanish evaluates paraphrase identification with sentence pairs that have high lexical overlap. The model must decide whether two Spanish sentences are paraphrases.",
      "long_description_es": "PAWS-X en espa\u00f1ol eval\u00faa la identificaci\u00f3n de par\u00e1frasis con pares de oraciones de alta superposici\u00f3n l\u00e9xica. El modelo debe decidir si dos oraciones en espa\u00f1ol son par\u00e1frasis.",
      "long_description_pt": "PAWS-X em espanhol avalia a identifica\u00e7\u00e3o de par\u00e1frases com pares de senten\u00e7as de alta sobreposi\u00e7\u00e3o lexical. O modelo deve decidir se duas senten\u00e7as em espanhol s\u00e3o par\u00e1frases.",
      "fewshot": 0,
      "URL": "https://huggingface.co/datasets/google-research-datasets/paws-x"
    },
    "teleia_pce": {
      "description": "Teleia PCE (Prueba de Conocimientos Espec\u00edficos)",
      "description_en": "Teleia PCE (Specific Knowledge Test)",
      "description_es": "Teleia PCE (Prueba de Conocimientos Espec\u00edficos)",
      "description_pt": "Teleia PCE (Prova de Conhecimentos Espec\u00edficos)",
      "long_description": "Teleia Spanish assessment suite \u2013 PCE (specific knowledge) subtask. Three-option multiple-choice questions that evaluate formal Spanish knowledge and usage.",
      "long_description_en": "Teleia Spanish assessment suite \u2013 PCE (specific knowledge) subtask. Three-option multiple-choice questions that evaluate formal Spanish knowledge and usage.",
      "long_description_es": "Suite de evaluaci\u00f3n de espa\u00f1ol Teleia \u2013 subtarea PCE (conocimientos espec\u00edficos). Preguntas de opci\u00f3n m\u00faltiple de tres opciones que eval\u00faan conocimiento y uso formal del espa\u00f1ol.",
      "long_description_pt": "Su\u00edte de avalia\u00e7\u00e3o de espanhol Teleia \u2013 subtarefa PCE (conhecimentos espec\u00edficos). Quest\u00f5es de m\u00faltipla escolha com tr\u00eas op\u00e7\u00f5es que avaliam conhecimento e uso formal do espanhol.",
      "fewshot": 0,
      "URL": "https://huggingface.co/datasets/migonsa/teleia"
    },
    "teleia_cervantes_ave": {
      "description": "Teleia Cervantes AVE assessment",
      "description_en": "Teleia Cervantes AVE assessment",
      "description_es": "Evaluaci\u00f3n Teleia Cervantes AVE",
      "description_pt": "Avalia\u00e7\u00e3o Teleia Cervantes AVE",
      "long_description": "Teleia Spanish assessment suite \u2013 Cervantes AVE subtask. Multiple-choice questions targeting Spanish language competency (reading and grammar) with dataset_name=cervantes_ave.",
      "long_description_en": "Teleia Spanish assessment suite \u2013 Cervantes AVE subtask. Multiple-choice questions targeting Spanish language competency (reading and grammar) with dataset_name=cervantes_ave.",
      "long_description_es": "Suite de evaluaci\u00f3n de espa\u00f1ol Teleia \u2013 subtarea Cervantes AVE. Preguntas de opci\u00f3n m\u00faltiple enfocadas en competencia en espa\u00f1ol (comprensi\u00f3n y gram\u00e1tica), con dataset_name=cervantes_ave.",
      "long_description_pt": "Su\u00edte de avalia\u00e7\u00e3o de espanhol Teleia \u2013 subtarefa Cervantes AVE. Quest\u00f5es de m\u00faltipla escolha focadas em compet\u00eancia em espanhol (leitura e gram\u00e1tica), com dataset_name=cervantes_ave.",
      "fewshot": 0,
      "URL": "https://huggingface.co/datasets/migonsa/teleia"
    },
    "teleia_siele": {
      "description": "Teleia SIELE (Servicio Internacional de Evaluaci\u00f3n de la Lengua Espa\u00f1ola)",
      "description_en": "Teleia SIELE (International Spanish Language Evaluation Service)",
      "description_es": "Teleia SIELE (Servicio Internacional de Evaluaci\u00f3n de la Lengua Espa\u00f1ola)",
      "description_pt": "Teleia SIELE (Servi\u00e7o Internacional de Avalia\u00e7\u00e3o da L\u00edngua Espanhola)",
      "long_description": "Teleia Spanish assessment suite \u2013 SIELE-inspired subtask. Three-option multiple-choice items covering comprehension and grammatical accuracy in Spanish.",
      "long_description_en": "Teleia Spanish assessment suite \u2013 SIELE-inspired subtask. Three-option multiple-choice items covering comprehension and grammatical accuracy in Spanish.",
      "long_description_es": "Suite de evaluaci\u00f3n de espa\u00f1ol Teleia \u2013 subtarea inspirada en SIELE. \u00cdtems de opci\u00f3n m\u00faltiple con tres opciones que cubren comprensi\u00f3n y correcci\u00f3n gramatical en espa\u00f1ol.",
      "long_description_pt": "Su\u00edte de avalia\u00e7\u00e3o de espanhol Teleia \u2013 subtarefa inspirada no SIELE. Itens de m\u00faltipla escolha com tr\u00eas op\u00e7\u00f5es cobrindo compreens\u00e3o e corre\u00e7\u00e3o gramatical em espanhol.",
      "fewshot": 0,
      "URL": "https://huggingface.co/datasets/migonsa/teleia"
    },
    "wnli_es": {
      "description": "Spanish WNLI (Winograd Natural Language Inference)",
      "description_en": "Spanish WNLI (Winograd Natural Language Inference)",
      "description_es": "WNLI en espa\u00f1ol (inferencia Winograd)",
      "description_pt": "WNLI em espanhol (infer\u00eancia Winograd)",
      "long_description": "Spanish WNLI is a localized Winograd-style NLI task. Given two sentences, the model decides whether the second follows (Verdadero) from the first or not (Falso).",
      "long_description_en": "Spanish WNLI is a localized Winograd-style NLI task. Given two sentences, the model decides whether the second follows (Verdadero) from the first or not (Falso).",
      "long_description_es": "WNLI en espa\u00f1ol es una tarea de NLI de estilo Winograd localizada. Dadas dos oraciones, el modelo decide si la segunda se sigue (Verdadero) de la primera o no (Falso).",
      "long_description_pt": "WNLI em espanhol \u00e9 uma tarefa de NLI no estilo Winograd. Dadas duas senten\u00e7as, o modelo decide se a segunda decorre (Verdadeiro) da primeira ou n\u00e3o (Falso).",
      "fewshot": 0,
      "URL": "https://huggingface.co/datasets/PlanTL-GOB-ES/wnli-es"
    }
  }
}
