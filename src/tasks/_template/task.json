{
  "name": "template_task",
  "display_name": "Template task",
  "description": "Template task - describe what this evaluates",
  "runner_entrypoint": "src.tasks._template.run:run_template_task",
  "task_format": "grouped",
  "provider_types": [
    "openai",
    "anthropic",
    "surus",
    "together"
  ],
  "tasks": [
    "multiple_choice_example",
    "structured_example",
    "freeform_example"
  ],
  "task_configs": {
    "multiple_choice_example": {
      "task_format": "multiple_choice",
      "dataset": {
        "dataset_path": "org/mcq-dataset",
        "split": "test"
      },
      "prompts": {
        "system": "You are a careful multiple-choice solver.",
        "user": "Question:\n{text}\n\nOptions:\n{choices}\n\nAnswer (label only):"
      }
    },
    "structured_example": {
      "task_format": "structured",
      "dataset": {
        "dataset_path": "org/structured-dataset",
        "split": "validation"
      },
      "prompts": {
        "system": "You are a structured extraction assistant.",
        "user": "Extract the fields from the text:\n{text}\n\nSchema:\n{schema}"
      }
    },
    "freeform_example": {
      "task_format": "freeform",
      "dataset": {
        "dataset_path": "org/freeform-dataset",
        "split": "train"
      },
      "prompts": {
        "system": "You are a helpful assistant.",
        "user": "Input:\n{text}\n\nPlease respond with a short answer."
      }
    }
  },
  "defaults": {
    "batch_size": 20,
    "log_samples": false,
    "temperature": 0.0,
    "max_tokens": 2048,
    "timeout": 120,
    "max_retries": 3
  },
  "metrics": {},
  "output": {
    "subdirectory": "template_task"
  },
  "metrics_manifest": []
}
