# Batch Configuration for Multiple Model Evaluation
# Add/remove models as needed

models:
  - name: "Qwen/Qwen3-4B-Instruct-2507"
    dtype: "float16"
    max_length: 16384
    tasks: "latam"
    device: "cuda"
    batch_size: "auto:4"
    output_path: "/home/mauro/dev/lm-evaluation-harness/output"
    limit: 10  # Remove this line for full evaluation

  - name: "google/gemma-2-2b-it"
    dtype: "float16"
    max_length: 8192
    tasks: "latam"
    device: "cuda"
    batch_size: "auto:4"
    output_path: "/home/mauro/dev/lm-evaluation-harness/output"

  - name: "meta-llama/Llama-3.2-3B-Instruct"
    dtype: "float16"
    max_length: 8192
    tasks: "latam"
    device: "cuda"
    batch_size: "auto:2"  # Smaller batch for larger model
    output_path: "/home/mauro/dev/lm-evaluation-harness/output"

  # Add more models here...
  # - name: "microsoft/phi-3-mini-4k-instruct"
  #   dtype: "float16"
  #   max_length: 4096
  #   tasks: "latam"
  #   device: "cuda"
  #   batch_size: "auto:4"
  #   output_path: "/home/mauro/dev/lm-evaluation-harness/output"

# Common configuration applied to all models
common:
  log_samples: true
  lm_eval_path: "/home/mauro/dev/lm-evaluation-harness"
  upload_script_path: "/home/mauro/dev/leaderboard"
  upload_script_name: "run_pipeline.py"
  
  wandb:
    entity: "surus-lat"
    project: "LATAM-leaderboard"
