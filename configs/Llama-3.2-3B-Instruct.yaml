# Benchy Configuration - Gemma Model
model:
  name: "meta-llama/Llama-3.2-3B-Instruct"
  dtype: "bfloat16" 
  max_length: 16384

evaluation:
  tasks: "latam"
  device: " "
  batch_size: "auto:4"
  output_path: "/home/mauro/dev/lm-evaluation-harness/output"
  log_samples: true
  cache_requests: true  # Enable request caching for speedup
  trust_remote_code: true

# Performance optimizations
performance:
  use_accelerate: true  # Set to true for multi-GPU acceleration
  num_gpus: 2           # Number of GPUs to use (when use_accelerate=true)
  mixed_precision: "no" # Options: "no", "fp16", "bf16"

wandb:
  entity: "surus-lat"
  project: "LATAM-leaderboard"

upload:
  script_path: "/home/mauro/dev/leaderboard"
  script_name: "run_pipeline.py"

# Logging configuration
logging:
  log_dir: "logs"  # Directory to store log files
  
venvs:
  lm_eval: "/home/mauro/dev/lm-evaluation-harness"
  leaderboard: "/home/mauro/dev/leaderboard"
