# Template: vLLM model config (single GPU)
# Copy to configs/models/<your_model>.yaml and edit.

model:
  name: "org/model-name"

vllm:
  # Base provider config file in configs/providers/
  provider_config: vllm_single_card
  overrides:
    # Common vLLM overrides
    port: 20502
    gpu_memory_utilization: 0.90
    max_model_len: 8192
    trust_remote_code: true
    # vllm_version: "0.10.2"         # Optional, uses default .venv if omitted
    # transformers_version: "4.51.3" # Optional

    # Optionally override provider capabilities for this stack
    # capabilities:
    #   supports_multimodal: false
    #   supports_logprobs: true

# Optional defaults applied to all tasks
# (can be overridden by task-specific defaults in task.json)
task_defaults:
  log_samples: true
  batch_size: 20
  cuda_devices: "0"

# Tasks or task groups (see configs/config.yaml task_groups)
tasks:
  - "latam_board"
  # - "spanish"
  # - "portuguese"

# Optional metadata tags used to restrict capabilities for this model
metadata:
  supports_multimodal: false
  supports_logprobs: true
  # supports_schema: true
  # supports_files: false
  # supports_streaming: false
