# Test configuration for Gemma with limited samples
model:
  name: "Qwen/Qwen3-4B-Instruct-2507"

evaluation:
  tasks_spanish: "latam_es"
  tasks_portuguese: "latam_pr"
  batch_size: "4"                # Further reduced batch for memory-intensive loglikelihood
  output_path: "/home/mauro/dev/benchmark_outputs"
  log_samples: true
  cache_requests: true
  trust_remote_code: true
  num_concurrent: 4               # Reduced concurrent requests to avoid memory pressure
  limit: 10                       # TEST mode - only 2 examples per task

# vLLM server configuration
vllm:
  host: "0.0.0.0"
  port: 20501
  tensor_parallel_size: 1
  max_model_len: 8192           # Match original working command
  gpu_memory_utilization: 0.7    # Reduced memory utilization to allow for loglikelihood computation
  enforce_eager: true
  limit_mm_per_prompt: '{"images": 0, "audios": 0}'
  kv_cache_memory: 12934271795    # Explicit KV cache memory allocation (as suggested by vLLM)
  hf_cache: "/mnt/Hiksemi-2Tb/.cache/huggingface"
  startup_timeout: 900           # Server startup timeout in seconds (15 minutes)
  cuda_devices: "3"             # CUDA devices to use (e.g., "3" or "2,3" for multiple)

wandb:
  entity: "surus-lat"
  project: "LATAM-leaderboard"

logging:
  log_dir: "logs"

venvs:
  lm_eval_spanish: "/home/mauro/dev/benchy/external/lm-evaluation-harness"
  lm_eval_portuguese: "/home/mauro/dev/benchy/external/portuguese-bench"
