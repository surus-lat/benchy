# Template: vLLM model config (multi GPU)
# Copy to configs/models/<your_model>.yaml and edit.

model:
  name: "org/model-name"

vllm:
  provider_config: vllm_two_cards
  overrides:
    port: 20502
    gpu_memory_utilization: 0.95
    tensor_parallel_size: 2
    max_model_len: 8192
    trust_remote_code: true
    # vllm_version: "0.10.2"

# Defaults applied to all tasks
# (task.json defaults can override these)
task_defaults:
  log_samples: true
  batch_size: 20
  cuda_devices: "0"

# Tasks or task groups
# See configs/config.yaml task_groups for shortcuts like "latam_board"
tasks:
  - "latam_board"

metadata:
  supports_multimodal: false
  supports_logprobs: true
