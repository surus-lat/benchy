# Template: vLLM provider config
# Save as configs/providers/<name>.yaml and reference via provider_config.

host: "0.0.0.0"
port: 20501
tensor_parallel_size: 1
max_model_len: 8192
gpu_memory_utilization: 0.95
max_num_seqs: 256
max_num_batched_tokens: 8192
enforce_eager: true
# Optional: cap multimodal inputs; if omitted, no --limit-mm-per-prompt flag is passed.
# limit_mm_per_prompt: '{"images": 1, "audios": 0}'
kv_cache_memory: 0
startup_timeout: 900
hf_cache: "/path/to/.cache/huggingface"
hf_token: ""

# API endpoint selection: "chat", "completions", or "auto"
api_endpoint: "auto"

# Optional vLLM version pinning
# vllm_version: "0.10.2"
# transformers_version: "4.51.3"

capabilities:
  supports_multimodal: false
  supports_schema: true
  supports_files: false
  supports_logprobs: true
  supports_streaming: false
  request_modes: ["chat", "completions"]
