# Benchy Configuration
# Configuration for ML model benchmarking with ZenML

model:
  name: "google/gemma-3-270m"
  dtype: "float16" 
  max_length: 8192

evaluation:
  tasks: "latam"
  device: "cuda"
  batch_size: "auto:4"
  output_path: "/home/mauro/dev/lm-evaluation-harness/output"
  log_samples: true
  limit: 10  # Limit number of examples per task (useful for testing, remove for full eval)

wandb:
  entity: "surus-lat"
  project: "LATAM-leaderboard"

upload:
  script_path: "/home/mauro/dev/leaderboard"
  script_name: "run_pipeline.py"

# Logging configuration
logging:
  log_dir: "logs"  # Directory to store log files
  # Log files are automatically named: benchy_{model_name}_{timestamp}.log

# Virtual environment paths
venvs:
  lm_eval: "/home/mauro/dev/lm-evaluation-harness"  # Path to lm-eval repo (venv activated explicitly)
  leaderboard: "/home/mauro/dev/leaderboard"  # Path to leaderboard repo (uv manages venv automatically)
