# Central Configuration File
# Global settings used throughout the benchy system

# Paths
paths:
  benchmark_outputs: "/home/mauro/dev/benchy/outputs/benchmark_outputs"
  reference_dir: "/home/mauro/dev/benchy/reference"
  publish_dir: "/home/mauro/dev/benchy/outputs/publish"
  logs: "logs"

# Hugging Face datasets
datasets:
  results: "LatamBoard/leaderboard-results"

# Default evaluation settings
evaluation:
  default_limit: null  # No limit by default, can be overridden by --limit

# Logging configuration
logging:
  log_dir: "logs"

# GPU Configuration
# Centralized GPU allocation for vLLM server and evaluation tasks
gpu_config:
  # vLLM server GPU allocation
  vllm:
    devices: "3"  # GPU devices for vLLM server (e.g., "3" or "2,3" or "1,2,3,4")
    # Alternative configurations:
    # devices: "2,3"    # Use GPUs 2 and 3 for vLLM
    # devices: "1,2,3,4" # Use all 4 GPUs for vLLM
  
  # Evaluation tasks GPU allocation  
  tasks:
    devices: "2"  # GPU device for evaluation tasks (e.g., "2" or "1")
    # Alternative configurations:
    # devices: "1"      # Use GPU 1 for tasks
    # devices: ""       # Use CPU only for tasks (current default)
  
  # Validation settings
  validation:
    check_gpu_availability: true  # Check if specified GPUs are available
    allow_overlap: false          # Prevent vLLM and tasks from using same GPUs

# Task Groups Configuration
# Define groups of tasks that can be used as shortcuts in model configs
task_groups:
  latam_board:
    description: "Complete LATAM evaluation suite"
    tasks:
      - "spanish"
      - "portuguese" 
      - "translation"
  
  # Example of additional task groups you can define:
  # language_only:
  #   description: "Language evaluation tasks only"
  #   tasks:
  #     - "spanish"
  #     - "portuguese"
  # 
  # translation_only:
  #   description: "Translation tasks only"
  #   tasks:
  #     - "translation"

# Leaderboard scoring configuration
leaderboard:
  # Categories to include in overall LATAM score calculation
  # These are automatically derived from task definitions below
  overall_score_categories:
    - "latam_es"    # Spanish category score
    - "latam_pr"    # Portuguese category score
    - "translation" # Translation category score
  
  # Normalize scores to 0-1 range (divide by 100) for specific tasks
  normalize_scores:
    - "translation"  # Normalize translation scores (CHRF from 0-100 to 0-1)
  
  # Task definitions for modular processing
  tasks:
    spanish:
      processor: "standard_results_processor"
      category_score_key: "latam_es"
      output_prefix: "spanish"
      subcategories:
        - name: "teleia"
          prefix: "teleia"
          filter_prefix: "teleia_"
    
    portuguese:
      processor: "portuguese_results_processor"
      category_score_key: "latam_pr"
      output_prefix: "portuguese"
    
    translation:
      processor: "translation_results_processor"
      category_score_key: "translation"
      output_prefix: "translation"
      metrics: ["chrf", "bleu"]  # Available metrics
      primary_metric: "chrf"     # Which metric to use for scoring
      exclude_tasks: ["translation"]  # Tasks to exclude from individual scores
    
    # Example of how to add a new task:
    # new_task:
    #   processor: "standard_results_processor"  # or "translation_results_processor"
    #   category_score_key: "new_task_category"
    #   output_prefix: "new_task"
    #   # Optional: subcategories for nested scoring
    #   subcategories:
    #     - name: "subcategory_name"
    #       prefix: "subcategory"
    #       filter_prefix: "subcategory_"