# Central Configuration File
# Global settings used throughout the benchy system

# Paths
paths:
  benchmark_outputs: "/home/mauro/dev/benchy/outputs/benchmark_outputs"
  reference_dir: "/home/mauro/dev/benchy/reference"
  publish_dir: "/home/mauro/dev/benchy/outputs/publish"
  logs: "logs"

# Hugging Face datasets
datasets:
  results: "LatamBoard/leaderboard-results"

# Default evaluation settings
evaluation:
  default_limit: null  # No limit by default, can be overridden by --limit

# Logging configuration
logging:
  log_dir: "logs"

# GPU Configuration
# Centralized GPU allocation for vLLM server and evaluation tasks
gpu_config:
  # vLLM server GPU allocation
  vllm:
    devices: "2,3"  # GPU devices for vLLM server (e.g., "3" or "2,3" or "1,2,3,4")
    # Alternative configurations:
    # devices: "3"      # Use single GPU 3 for vLLM
    # devices: "1,2,3,4" # Use all 4 GPUs for vLLM
  
  # Evaluation tasks GPU allocation  
  tasks:
    devices: "0"  # GPU device for evaluation tasks (e.g., "1" or "0")
    # Alternative configurations:
    # devices: "0"      # Use GPU 0 for tasks
    # devices: ""       # Use CPU only for tasks (current default)
  
  # Validation settings
  validation:
    check_gpu_availability: true  # Check if specified GPUs are available
    allow_overlap: false          # Prevent vLLM and tasks from using same GPUs

# Task Groups Configuration
# Define groups of tasks that can be used as shortcuts in model configs
task_groups:
  latam_board:
    description: "Complete LATAM evaluation suite"
    tasks:
      - "spanish"
      - "portuguese" 
      - "translation"
      - "structured_extraction"
  
  structured_only:
    description: "Structured data extraction only"
    tasks:
      - "structured_extraction"
  
  image_extraction_only:
    description: "Image extraction (multimodal) only"
    tasks:
      - "image_extraction"
  
  # Example of additional task groups you can define:
  # language_only:
  #   description: "Language evaluation tasks only"
  #   tasks:
  #     - "spanish"
  #     - "portuguese"
  # 
  # translation_only:
  #   description: "Translation tasks only"
  #   tasks:
  #     - "translation"

# Leaderboard scoring configuration
leaderboard:
  # Categories to include in overall LATAM score calculation
  # These are automatically derived from task definitions below
  overall_score_categories:
    - "latam_es"    # Spanish category score
    - "latam_pr"    # Portuguese category score
    - "translation" # Translation category score
    - "structured_extraction"  # Uncomment to include structured extraction in overall score
  
  # Normalize scores to 0-1 range (divide by 100) for specific tasks
  normalize_scores:
    - "translation"  # Normalize translation scores (CHRF from 0-100 to 0-1)
  
  # Task definitions for modular processing
  tasks:
    spanish:
      processor: "standard_results_processor"
      category_score_key: "latam_es"
      output_prefix: "spanish"
      subcategories:
        - name: "teleia"
          prefix: "teleia"
          filter_prefix: "teleia_"
    
    portuguese:
      processor: "portuguese_results_processor"
      category_score_key: "latam_pr"
      output_prefix: "portuguese"
    
    translation:
      processor: "translation_results_processor"
      category_score_key: "translation"
      output_prefix: "translation"
      metrics: ["chrf", "bleu", "comet"]  # Available metrics
      primary_metric: "comet"     # Which metric to use for scoring
      exclude_tasks: ["translation"]  # Tasks to exclude from individual scores
    
    structured_extraction:
      processor: "structured_extraction_results_processor"
      category_score_key: "structured_extraction"
      output_prefix: "structured_extraction"
      metrics: ["composite_score", "eqs", "f1_partial"]  # Available metrics
      primary_metric: "composite_score"  # Which metric to use for scoring (composite_score_stats.mean)
    
    image_extraction:
      processor: "structured_extraction_results_processor"
      category_score_key: "image_extraction"
      output_prefix: "image_extraction"
      metrics: ["composite_score", "eqs", "f1_partial"]  # Available metrics
      primary_metric: "composite_score"  # Which metric to use for scoring (composite_score_stats.mean)
    
    # Example of how to add a new task:
    # new_task:
    #   processor: "standard_results_processor"  # or "translation_results_processor"
    #   category_score_key: "new_task_category"
    #   output_prefix: "new_task"
    #   # Optional: subcategories for nested scoring
    #   subcategories:
    #     - name: "subcategory_name"
    #       prefix: "subcategory"
    #       filter_prefix: "subcategory_"