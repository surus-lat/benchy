# Base vLLM configuration for single GPU setup
# This config contains all default values for vLLM server

host: "0.0.0.0"
port: 20501
tensor_parallel_size: 1
max_model_len: 8192
gpu_memory_utilization: 0.95
enforce_eager: true
limit_mm_per_prompt: '{"images": 0, "audios": 0}'
kv_cache_memory: 0
max_num_seqs: 256
max_num_batched_tokens: 8192
hf_cache: "/mnt/Hiksemi-2Tb/.cache/huggingface"
hf_token: ""
startup_timeout: 900
# cuda_devices: "3"  # Now configured centrally in configs/config.yaml under gpu_config.vllm.devices

# API endpoint selection (for structured extraction tasks)
# - "chat": Use chat completions API (requires chat template)
# - "completions": Use completions API (works with any model)
# - "auto": Try chat first, fallback to completions on error (recommended for vLLM)
api_endpoint: "auto"

# Additional parameters you can override
#vllm_version: "0.8.0" # uses latest vLLM version from main project environment overwise
#multimodal: false # true by default
