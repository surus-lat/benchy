# vLLM configuration for two GPU setup with tensor parallelism
# This config contains all default values for vLLM server with 2 GPUs

host: "0.0.0.0"
port: 20502
tensor_parallel_size: 2
max_model_len: 15696
gpu_memory_utilization: 0.95
enforce_eager: true
kv_cache_memory: 0
max_num_seqs: 256
max_num_batched_tokens: 8192
hf_cache: "/mnt/Hiksemi-2Tb/.cache/huggingface"
hf_token: ""
startup_timeout: 900
# cuda_devices: "2,3"  # Now configured centrally in configs/config.yaml under gpu_config.vllm.devices

# API endpoint selection (for structured extraction tasks)
# - "chat": Use chat completions API (requires chat template)
# - "completions": Use completions API (works with any model)
# - "auto": Try chat first, fallback to completions on error (recommended for vLLM)
api_endpoint: "auto"

# Additional parameters you can override
#vllm_version: "0.8.0" # uses latest vLLM version from main project environment overwise
#multimodal: false # true by default

# Provider capability flags for compatibility checks
capabilities:
  supports_multimodal: true
  supports_schema: true
  supports_files: true
  supports_logprobs: true
  supports_streaming: false
  request_modes: ["chat", "completions"]
