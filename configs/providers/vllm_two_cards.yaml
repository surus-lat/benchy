# vLLM configuration for two GPU setup with tensor parallelism
# This config contains all default values for vLLM server with 2 GPUs

host: "0.0.0.0"
port: 20502
tensor_parallel_size: 2
max_model_len: 8192
gpu_memory_utilization: 0.95
enforce_eager: true
limit_mm_per_prompt: '{"images": 0, "audios": 0}'
kv_cache_memory: 0
max_num_seqs: 256
max_num_batched_tokens: 8192
hf_cache: "/mnt/Hiksemi-2Tb/.cache/huggingface"
hf_token: ""
startup_timeout: 900
# cuda_devices: "2,3"  # Now configured centrally in configs/config.yaml under gpu_config.vllm.devices
# Additional parameters you can override
#vllm_version: "0.8.0" # uses latest vLLM version from main project environment overwise
#multimodal: false # true by default

