# Benchy Configuration - vLLM-based Gemma Model Evaluation
model:
  name: "google/gemma-3n-E4B-it"

evaluation:
  tasks_spanish: "latam_es"      # Spanish evaluation tasks
  tasks_portuguese: "latam_pt"   # Portuguese evaluation tasks
  batch_size: "20"               # Batch size for API mode
  output_path: "/home/mauro/dev/lm-evaluation-harness/output"
  log_samples: true
  cache_requests: true           # Enable request caching for speedup
  trust_remote_code: true
  num_concurrent: 20             # Number of concurrent API requests
  limit: 5                     # Uncomment for testing (TEST mode)

# vLLM server configuration
vllm:
  host: "0.0.0.0"                # Host to bind server to
  port: 8000                     # Port for vLLM server
  tensor_parallel_size: 1        # Number of GPUs for tensor parallelism (-tp)
  max_model_len: 8192           # Maximum model length
  gpu_memory_utilization: 0.6    # GPU memory utilization
  enforce_eager: true            # Enforce eager execution (better compatibility)
  limit_mm_per_prompt: '{"images": 0, "audios": 0}'  # Disable multimodal
  hf_cache: "/home/mauro/.cache/huggingface"          # HF cache directory
  # hf_token: "hf_..."           # Uncomment and set your HF token if needed

wandb:
  entity: "surus-lat"
  project: "LATAM-leaderboard"

upload:
  script_path: "/home/mauro/dev/leaderboard"
  script_name: "run_pipeline.py"

# Logging configuration
logging:
  log_dir: "logs"

# Virtual environment paths
venvs:
  lm_eval: "/home/mauro/dev/lm-evaluation-harness"
  leaderboard: "/home/mauro/dev/leaderboard"