{
  "task_groups": {
    "latam_pr": {
      "name": "Portuguese LATAM",
      "description": "Portuguese language tasks for Latin America",
      "description_en": "Portuguese language tasks for Latin America",
      "description_es": "Tareas en portugu\u00e9s para Am\u00e9rica Latina",
      "description_pt": "Tarefas em portugu\u00eas para a Am\u00e9rica Latina",
      "long_description": "Suite of selected tasks from the Portuguese LATAM group designed to evaluate the performance of models in the Portuguese language. Based on the work of the Open Portuguese LLM Leaderboard, these tasks were carefully selected to measure the capabilities of language models in understanding Portuguese. The tasks cover a wide range of linguistic abilities, from basic comprehension to complex reasoning in Portuguese. The evaluation suite includes tasks like ASSIN2 for textual entailment and semantic similarity, BLUEX for university entrance exams, and ENEM for standardized testing comprehension. This comprehensive set of benchmarks helps assess how well language models can process and generate Portuguese text across different contexts and difficulty levels.",
      "long_description_en": "Suite of selected tasks from the Portuguese LATAM group designed to evaluate the performance of models in the Portuguese language. Based on the work of the Open Portuguese LLM Leaderboard, these tasks were carefully selected to measure the capabilities of language models in understanding Portuguese. The tasks cover a wide range of linguistic abilities, from basic comprehension to complex reasoning in Portuguese. The evaluation suite includes tasks like ASSIN2 for textual entailment and semantic similarity, BLUEX for university entrance exams, and ENEM for standardized testing comprehension. This comprehensive set of benchmarks helps assess how well language models can process and generate Portuguese text across different contexts and difficulty levels.",
      "long_description_es": "Conjunto de tareas seleccionadas del grupo LATAM en portugu\u00e9s, dise\u00f1ado para evaluar el desempe\u00f1o de los modelos en dicho idioma. Basado en el trabajo del Open Portuguese LLM Leaderboard, estas tareas fueron cuidadosamente elegidas para medir la capacidad de los modelos de lenguaje para comprender el portugu\u00e9s. Cubren un amplio espectro de habilidades, desde comprensi\u00f3n b\u00e1sica hasta razonamiento complejo. La suite incluye ASSIN2 para inferencia textual y similitud sem\u00e1ntica, BLUEX para ex\u00e1menes de ingreso a la universidad y ENEM para comprensi\u00f3n en pruebas estandarizadas. Este conjunto permite evaluar c\u00f3mo los modelos procesan y generan texto en portugu\u00e9s en distintos contextos y niveles de dificultad.",
      "long_description_pt": "Conjunto de tarefas selecionadas do grupo LATAM em portugu\u00eas, projetado para avaliar o desempenho dos modelos no idioma. Com base no Open Portuguese LLM Leaderboard, essas tarefas foram escolhidas para medir a capacidade dos modelos de compreender o portugu\u00eas. As tarefas cobrem desde compreens\u00e3o b\u00e1sica at\u00e9 racioc\u00ednio complexo. A su\u00edte inclui ASSIN2 para infer\u00eancia textual e similaridade sem\u00e2ntica, BLUEX para vestibulares e ENEM para compreens\u00e3o em exames padronizados. Esse conjunto avalia como os modelos processam e geram texto em portugu\u00eas em diversos contextos e n\u00edveis de dificuldade.",
      "repository": "https://github.com/surus-lat/portuguese-bench",
      "subtasks": [
        "assin2_rte",
        "assin2_sts",
        "bluex",
        "enem_challenge",
        "oab_exams"
      ]
    },
    "latam_es": {
      "name": "Spanish LATAM",
      "description": "Spanish language tasks for Latin America",
      "description_en": "Spanish language tasks for Latin America",
      "description_es": "Tareas en espa\u00f1ol para Am\u00e9rica Latina",
      "description_pt": "Tarefas em espanhol para a Am\u00e9rica Latina",
      "long_description": "Suite of selected tasks from the Spanish Bench available in the lm-evaluation-harness from the team at IberoBench (https://aclanthology.org/2025.coling-main.699/) and SomosNLP's Spanish Leaderboard (https://github.com/somosnlp/lm-evaluation-harness) designed to evaluate the performance of models in the Spanish language. The tasks cover a wide range of linguistic abilities, from basic comprehension to complex reasoning in Spanish. The evaluation suite includes tasks like COPA for choice of plausible alternatives, ESCOLA for Spanish Corpus of Linguistic Acceptability, MGSM for Multilingual Grade School Math, OpenBookQA for open-domain question answering, PAWS for paraphrase adversaries from word scrambling, TELEIA for Teleia Spanish language assessment, WNLI for Winograd Natural Language Inference, and XNLI for Cross-lingual Natural Language Inference. This comprehensive set of benchmarks helps assess how well language models can process and generate Spanish text across different contexts and difficulty levels.",
      "long_description_en": "Suite of selected tasks from the Spanish Bench available in the lm-evaluation-harness from the team at IberoBench (https://aclanthology.org/2025.coling-main.699/) and SomosNLP's Spanish Leaderboard (https://github.com/somosnlp/lm-evaluation-harness) designed to evaluate the performance of models in the Spanish language. The tasks cover a wide range of linguistic abilities, from basic comprehension to complex reasoning in Spanish. The evaluation suite includes tasks like COPA for choice of plausible alternatives, ESCOLA for Spanish Corpus of Linguistic Acceptability, MGSM for Multilingual Grade School Math, OpenBookQA for open-domain question answering, PAWS for paraphrase adversaries from word scrambling, TELEIA for Teleia Spanish language assessment, WNLI for Winograd Natural Language Inference, and XNLI for Cross-lingual Natural Language Inference. This comprehensive set of benchmarks helps assess how well language models can process and generate Spanish text across different contexts and difficulty levels.",
      "long_description_es": "Conjunto de tareas seleccionadas del Spanish Bench del lm-evaluation-harness del equipo de IberoBench (https://aclanthology.org/2025.coling-main.699/) y del Spanish Leaderboard de SomosNLP (https://github.com/somosnlp/lm-evaluation-harness), dise\u00f1ado para evaluar el desempe\u00f1o de modelos en lengua espa\u00f1ola. Cubre habilidades desde comprensi\u00f3n b\u00e1sica hasta razonamiento complejo. Incluye COPA (elecci\u00f3n de alternativas plausibles), EsCoLA (aceptabilidad ling\u00fc\u00edstica), MGSM (matem\u00e1tica escolar multiling\u00fce), OpenBookQA (preguntas de ciencia de dominio abierto), PAWS (par\u00e1frasis con alta superposici\u00f3n l\u00e9xica), TELEIA (evaluaci\u00f3n del espa\u00f1ol), WNLI (inferencia Winograd) y XNLI (inferencia multiling\u00fce). Este conjunto permite medir c\u00f3mo los modelos procesan y generan texto en espa\u00f1ol en diversos contextos y niveles de dificultad.",
      "long_description_pt": "Conjunto de tarefas selecionadas do Spanish Bench no lm-evaluation-harness da equipe IberoBench (https://aclanthology.org/2025.coling-main.699/) e do Spanish Leaderboard da SomosNLP (https://github.com/somosnlp/lm-evaluation-harness), projetado para avaliar o desempenho de modelos em l\u00edngua espanhola. Cobre habilidades desde compreens\u00e3o b\u00e1sica at\u00e9 racioc\u00ednio complexo. Inclui COPA (escolha de alternativas plaus\u00edveis), EsCoLA (aceitabilidade lingu\u00edstica), MGSM (matem\u00e1tica escolar multil\u00edngue), OpenBookQA (perguntas de ci\u00eancia de dom\u00ednio aberto), PAWS (par\u00e1frases com alta sobreposi\u00e7\u00e3o lexical), TELEIA (avalia\u00e7\u00e3o do espanhol), WNLI (infer\u00eancia Winograd) e XNLI (infer\u00eancia multil\u00edngue). Esse conjunto mede como os modelos processam e geram texto em espanhol em diferentes contextos e n\u00edveis de dificuldade.",
      "repository": "https://github.com/surus-lat/lm-evaluation-harness-surus",
      "subtasks": [
        "copa_es",
        "escola",
        "mgsm_direct_es_spanish_bench",
        "openbookqa_es",
        "paws_es_spanish_bench",
        "teleia_pce",
        "teleia_cervantes_ave",
        "teleia_siele",
        "wnli_es"
      ]
    },
    "translation": {
      "name": "Translation",
      "description": "Machine translation evaluation tasks",
      "description_en": "Machine translation evaluation tasks",
      "description_es": "Tareas de evaluaci\u00f3n de traducci\u00f3n autom\u00e1tica",
      "description_pt": "Tarefas de avalia\u00e7\u00e3o de tradu\u00e7\u00e3o autom\u00e1tica",
      "long_description": "Comprehensive machine translation evaluation suite combining FLORES+ and OPUS-100 datasets to assess multilingual translation capabilities. FLORES+ provides high-quality translations across Spanish, Portuguese, English, French, Italian, German, Hindi, Arabic and Chinese, covering 30 language pairs focused on Latin American and Iberian languages. OPUS-100 contributes English-Spanish and English-Portuguese translation pairs from a large-scale multilingual corpus. Both datasets evaluate translation quality using COMET, BLEU and chrF metrics, with COMET as the primary reported metric. This evaluation suite measures models' ability to accurately translate between languages while maintaining semantic meaning and linguistic fluency across diverse text domains.",
      "long_description_en": "Comprehensive machine translation evaluation suite combining FLORES+ and OPUS-100 datasets to assess multilingual translation capabilities. FLORES+ provides high-quality translations across Spanish, Portuguese, English, French, Italian, German, hindi, arabic and chinese, covering 30 language pairs focused on Latin American and Iberian languages. OPUS-100 contributes English-Spanish and English-Portuguese translation pairs from a large-scale multilingual corpus. Both datasets evaluate translation quality using COMET, BLEU and chrF metrics, with COMET as the primary reported metric. This evaluation suite measures models' ability to accurately translate between languages while maintaining semantic meaning and linguistic fluency across diverse text domains.",
      "long_description_es": "Suite integral de evaluaci\u00f3n de traducci\u00f3n autom\u00e1tica que combina los conjuntos de datos FLORES+ y OPUS-100 para evaluar capacidades de traducci\u00f3n multiling\u00fce. FLORES+ proporciona traducciones de alta calidad entre espa\u00f1ol, portugu\u00e9s, ingl\u00e9s, franc\u00e9s, italiano, alem\u00e1n, indio, arabe y chino, cubriendo 30 pares de idiomas enfocados en lenguas latinoamericanas e ib\u00e9ricas. OPUS-100 aporta pares de traducci\u00f3n ingl\u00e9s-espa\u00f1ol e ingl\u00e9s-portugu\u00e9s de un corpus multiling\u00fce a gran escala. Ambos conjuntos eval\u00faan la calidad de traducci\u00f3n usando m\u00e9tricas COMET, BLEU y chrF, con COMET como m\u00e9trica principal reportada. Esta suite mide la capacidad de los modelos para traducir con precisi\u00f3n entre idiomas manteniendo el significado sem\u00e1ntico y la fluidez ling\u00fc\u00edstica en diversos dominios textuales.",
      "long_description_pt": "Su\u00edte abrangente de avalia\u00e7\u00e3o de tradu\u00e7\u00e3o autom\u00e1tica combinando os conjuntos FLORES+ e OPUS-100 para avaliar capacidades de tradu\u00e7\u00e3o multil\u00edngue. FLORES+ fornece tradu\u00e7\u00f5es de alta qualidade entre espanhol, portugu\u00eas, ingl\u00eas, franc\u00eas, italiano, alem\u00e3o, indiano, \u00e1rabe e chin\u00eas, cobrindo 30 pares de idiomas focados em l\u00ednguas latino-americanas e ib\u00e9ricas. OPUS-100 contribui com pares de tradu\u00e7\u00e3o ingl\u00eas-espanhol e ingl\u00eas-portugu\u00eas de um corpus multil\u00edngue em larga escala. Ambos os conjuntos avaliam a qualidade da tradu\u00e7\u00e3o usando m\u00e9tricas COMET, BLEU e chrF, com COMET como m\u00e9trica principal reportada. Esta su\u00edte mede a capacidade dos modelos de traduzir com precis\u00e3o entre idiomas mantendo o significado sem\u00e2ntico e a flu\u00eancia lingu\u00edstica em diversos dom\u00ednios textuais.",
      "repository": "https://github.com/surus-lat/lm-evaluation-harness-surus",
      "subtasks": [
        "opus",
        "flores"
      ]
    },
    "structured_extraction": {
      "name": "Structured Extraction",
      "description": "Structured data extraction from unstructured text",
      "description_en": "Structured data extraction from unstructured text",
      "description_es": "Extracci\u00f3n de datos estructurados de texto no estructurado",
      "description_pt": "Extra\u00e7\u00e3o de dados estruturados de texto n\u00e3o estruturado",
      "long_description": "Comprehensive evaluation suite for AI systems' capabilities in extracting structured information from unstructured text and returning it in JSON format. This task group focuses on testing models' ability to parse diverse document types and extract all relevant information while adhering to provided JSON schemas. The evaluation covers multiple domains including medical records, e-commerce listings, business documents, travel itineraries, media content, technology specifications, and manufacturing reports. This benchmark assesses both comprehension accuracy and structured output generation capabilities, measuring how well language models can transform unstructured text into precise, schema-compliant JSON responses.",
      "long_description_en": "Comprehensive evaluation suite for AI systems' capabilities in extracting structured information from unstructured text and returning it in JSON format. This task group focuses on testing models' ability to parse diverse document types and extract all relevant information while adhering to provided JSON schemas. The evaluation covers multiple domains including medical records, e-commerce listings, business documents, travel itineraries, media content, technology specifications, and manufacturing reports. This benchmark assesses both comprehension accuracy and structured output generation capabilities, measuring how well language models can transform unstructured text into precise, schema-compliant JSON responses.",
      "long_description_es": "Suite integral de evaluaci\u00f3n para las capacidades de los sistemas de IA en extraer informaci\u00f3n estructurada de texto no estructurado y devolverla en formato JSON. Este grupo de tareas se enfoca en probar la capacidad de los modelos para analizar diversos tipos de documentos y extraer toda la informaci\u00f3n relevante mientras se adhieren a esquemas JSON proporcionados. La evaluaci\u00f3n cubre m\u00faltiples dominios incluyendo registros m\u00e9dicos, listados de comercio electr\u00f3nico, documentos comerciales, itinerarios de viaje, contenido multimedia, especificaciones tecnol\u00f3gicas e informes de manufactura. Este benchmark eval\u00faa tanto la precisi\u00f3n de comprensi\u00f3n como las capacidades de generaci\u00f3n de salida estructurada, midiendo qu\u00e9 tan bien los modelos de lenguaje pueden transformar texto no estructurado en respuestas JSON precisas y compatibles con esquemas.",
      "long_description_pt": "Su\u00edte abrangente de avalia\u00e7\u00e3o para as capacidades dos sistemas de IA em extrair informa\u00e7\u00f5es estruturadas de texto n\u00e3o estruturado e retorn\u00e1-las em formato JSON. Este grupo de tarefas foca em testar a capacidade dos modelos de analisar diversos tipos de documentos e extrair toda a informa\u00e7\u00e3o relevante enquanto aderem a esquemas JSON fornecidos. A avalia\u00e7\u00e3o cobre m\u00faltiplos dom\u00ednios incluindo registros m\u00e9dicos, listagens de e-commerce, documentos comerciais, itiner\u00e1rios de viagem, conte\u00fado de m\u00eddia, especifica\u00e7\u00f5es tecnol\u00f3gicas e relat\u00f3rios de manufatura. Este benchmark avalia tanto a precis\u00e3o de compreens\u00e3o quanto as capacidades de gera\u00e7\u00e3o de sa\u00edda estruturada, medindo qu\u00e3o bem os modelos de linguagem podem transformar texto n\u00e3o estruturado em respostas JSON precisas e compat\u00edveis com esquemas.",
      "repository": "https://github.com/surus-lat/benchy",
      "subtasks": [
        "paraloq"
      ]
    },
    "image_extraction": {
      "name": "Image Extraction",
      "description": "Structured data extraction from document images",
      "description_en": "Structured data extraction from document images",
      "description_es": "Extracci\u00f3n de datos estructurados de im\u00e1genes de documentos",
      "description_pt": "Extra\u00e7\u00e3o de dados estruturados de imagens de documentos",
      "long_description": "Evaluates vision-language models on structured data extraction from document images such as invoices, forms, receipts, and other business documents. Given an image and a JSON schema, the model must extract structured data that conforms to the schema. This task measures how accurately models can read and interpret visual documents, with particular emphasis on numeric accuracy for financial and business documents. The evaluation uses comprehensive metrics including document extraction score, field-level F1 scores, schema validity, and hallucination detection. The task supports dataset-specific configuration for normalization rules, field matching thresholds, and metric weights, allowing adaptation to different document types and extraction requirements.",
      "long_description_en": "Evaluates vision-language models on structured data extraction from document images such as invoices, forms, receipts, and other business documents. Given an image and a JSON schema, the model must extract structured data that conforms to the schema. This task measures how accurately models can read and interpret visual documents, with particular emphasis on numeric accuracy for financial and business documents. The evaluation uses comprehensive metrics including document extraction score, field-level F1 scores, schema validity, and hallucination detection. The task supports dataset-specific configuration for normalization rules, field matching thresholds, and metric weights, allowing adaptation to different document types and extraction requirements.",
      "long_description_es": "Eval\u00faa modelos de visi\u00f3n-lenguaje en la extracci\u00f3n de datos estructurados de im\u00e1genes de documentos como facturas, formularios, recibos y otros documentos comerciales. Dada una imagen y un esquema JSON, el modelo debe extraer datos estructurados que se ajusten al esquema. Esta tarea mide qu\u00e9 tan precisamente los modelos pueden leer e interpretar documentos visuales, con \u00e9nfasis particular en la precisi\u00f3n num\u00e9rica para documentos financieros y comerciales. La evaluaci\u00f3n utiliza m\u00e9tricas integrales que incluyen puntuaci\u00f3n de extracci\u00f3n de documentos, puntuaciones F1 a nivel de campo, validez de esquema y detecci\u00f3n de alucinaciones. La tarea admite configuraci\u00f3n espec\u00edfica del conjunto de datos para reglas de normalizaci\u00f3n, umbrales de coincidencia de campos y pesos de m\u00e9tricas, permitiendo la adaptaci\u00f3n a diferentes tipos de documentos y requisitos de extracci\u00f3n.",
      "long_description_pt": "Avalia modelos de vis\u00e3o-linguagem na extra\u00e7\u00e3o de dados estruturados de imagens de documentos como faturas, formul\u00e1rios, recibos e outros documentos comerciais. Dada uma imagem e um esquema JSON, o modelo deve extrair dados estruturados que se ajustem ao esquema. Esta tarefa mede qu\u00e3o precisamente os modelos podem ler e interpretar documentos visuais, com \u00eanfase particular na precis\u00e3o num\u00e9rica para documentos financeiros e comerciais. A avalia\u00e7\u00e3o utiliza m\u00e9tricas abrangentes incluindo pontua\u00e7\u00e3o de extra\u00e7\u00e3o de documentos, pontua\u00e7\u00f5es F1 em n\u00edvel de campo, validade de esquema e detec\u00e7\u00e3o de alucina\u00e7\u00f5es. A tarefa suporta configura\u00e7\u00e3o espec\u00edfica do conjunto de dados para regras de normaliza\u00e7\u00e3o, limites de correspond\u00eancia de campos e pesos de m\u00e9tricas, permitindo adapta\u00e7\u00e3o a diferentes tipos de documentos e requisitos de extra\u00e7\u00e3o.",
      "repository": "https://github.com/surus-lat/benchy",
      "subtasks": []
    }
  }
}
